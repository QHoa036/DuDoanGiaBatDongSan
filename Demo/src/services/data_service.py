#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
D·ªãch v·ª• D·ªØ li·ªáu - X·ª≠ l√Ω vi·ªác t·∫£i d·ªØ li·ªáu, x·ª≠ l√Ω v√† c√°c ho·∫°t ƒë·ªông m√¥ h√¨nh
"""

# MARK: - Th∆∞ vi·ªán

import os
import sys
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any
import streamlit as st
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
from pyspark.ml.regression import GBTRegressor
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from ..models.property_model import Property, PredictionResult
from ..utils.logger_utils import get_logger
from ..utils.spark_utils import get_spark_session, configure_spark_logging
from ..utils.session_utils import save_model_metrics, get_model_metrics, metrics_exist

# Tr·∫£ v·ªÅ True n·∫øu scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t
sklearn_available = False
try:
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.metrics import r2_score, mean_squared_error
    sklearn_available = True
except ImportError:
    st.warning("üîî Th∆∞ vi·ªán scikit-learn kh√¥ng c√≥ s·∫µn. C√†i ƒë·∫∑t b·∫±ng c√°ch ch·∫°y: pip install scikit-learn")
    st.info("üìö S·∫Ω s·ª≠ d·ª•ng ch·∫ø ƒë·ªô d·ª± ph√≤ng ƒë∆°n gi·∫£n h∆°n.")

# MARK: - C·∫•u h√¨nh

utils_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'utils')
if utils_path not in sys.path:
    sys.path.append(utils_path)

# MARK: - L·ªõp d·ªãch v·ª• d·ªØ li·ªáu

class DataService:
    """
    L·ªõp d·ªãch v·ª• ch·ªãu tr√°ch nhi·ªám cho c√°c ho·∫°t ƒë·ªông d·ªØ li·ªáu v√† ch·ª©c nƒÉng ML
    Cung c·∫•p c√°c ph∆∞∆°ng th·ª©c ƒë·ªÉ t·∫£i, x·ª≠ l√Ω d·ªØ li·ªáu v√† c√°c ho·∫°t ƒë·ªông m√¥ h√¨nh
    """

    # MARK: - Kh·ªüi t·∫°o

    def __init__(self):
        """
        Kh·ªüi t·∫°o d·ªãch v·ª• d·ªØ li·ªáu
        """
        configure_spark_logging()

        # Kh·ªüi t·∫°o logger
        self.logger = get_logger(__name__)

        # Kh·ªüi t·∫°o phi√™n Spark (lazy loading)
        self._spark = None
        self._model = None
        self._accuracy = 0.0
        self._rmse = 0.0
        self._r2 = 0.0
        self._data = None
        self._spark_df = None
        self._using_fallback = False
        self._fallback_model = None
        self._fallback_features = None

        # Define feature column names
        self._feature_columns = {
            'area': 'area (m2)',
            'street': 'street (m)'
        }

    # MARK: - Thu·ªôc t√≠nh

    @property
    def spark(self):
        """
        L·∫•y phi√™n Spark (kh·ªüi t·∫°o khi truy c·∫≠p l·∫ßn ƒë·∫ßu)
        """
        if self._spark is None:
            self._spark = get_spark_session(app_name="VNRealEstatePricePrediction")
        return self._spark

    @property
    def model_metrics(self) -> Dict[str, float]:
        """
        L·∫•y c√°c ch·ªâ s·ªë c·ªßa m√¥ h√¨nh hi·ªán t·∫°i
        """
        # ∆Øu ti√™n s·ª≠ d·ª•ng metrics t·ª´ session state n·∫øu c√≥
        if metrics_exist():
            session_metrics = get_model_metrics()
            # Tr·∫£ v·ªÅ metrics t·ª´ session state, ƒë·∫£m b·∫£o tr∆∞·ªùng 'accuracy' cho t∆∞∆°ng th√≠ch ng∆∞·ª£c
            metrics = {
                'r2': session_metrics.get('r2', 0.0),
                'accuracy': session_metrics.get('r2', 0.0),  # ƒê·∫£m b·∫£o t∆∞∆°ng th√≠ch ng∆∞·ª£c
                'rmse': session_metrics.get('rmse', 0.0)
            }
            return metrics

        # N·∫øu kh√¥ng c√≥ trong session state, s·ª≠ d·ª•ng c√°c gi√° tr·ªã trong ƒë·ªëi t∆∞·ª£ng
        return {
            'r2': self._r2,
            'accuracy': self._accuracy,
            'rmse': self._rmse
        }

    # MARK: - T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu

    @st.cache_data
    def load_data(_self, file_path: Optional[str] = None) -> pd.DataFrame:
        """
        T·∫£i d·ªØ li·ªáu t·ª´ file CSV
        """
        if _self._data is not None:
            return _self._data

        try:
            # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
            if file_path is None:
                # T√¨m th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
                base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu trong th∆∞ m·ª•c src/data
                file_path = os.path.join(base_dir, 'data', 'final_data_cleaned.csv')

                # Ki·ªÉm tra xem file c√≥ t·ªìn t·∫°i kh√¥ng
                if not os.path.exists(file_path):
                    # Th·ª≠ t√¨m file ·ªü v·ªã tr√≠ kh√°c
                    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    alternate_paths = [
                        os.path.join(project_root, 'Demo', 'data', 'final_data_cleaned.csv')
                    ]

                    for alt_path in alternate_paths:
                        if os.path.exists(alt_path):
                            file_path = alt_path
                            _self.logger.info(f"T√¨m th·∫•y file d·ªØ li·ªáu t·∫°i: {file_path}")
                            break
                    else:
                        # N·∫øu kh√¥ng t√¨m th·∫•y file ·ªü b·∫•t k·ª≥ v·ªã tr√≠ n√†o
                        raise FileNotFoundError(
                            f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i: {file_path}\n"
                            "Vui l√≤ng ƒë·∫£m b·∫£o r·∫±ng:\n"
                            "1. B·∫°n ƒë√£ t·∫£i d·ªØ li·ªáu v√† ƒë·∫∑t trong th∆∞ m·ª•c Demo/data/\n"
                            "2. File ƒë∆∞·ª£c ƒë·∫∑t t√™n ch√≠nh x√°c l√† 'final_data_cleaned.csv'\n"
                            "3. B·∫°n ƒë√£ ch·∫°y to√†n b·ªô quy tr√¨nh t·ª´ ƒë·∫ßu b·∫±ng run_demo.sh"
                        )

            # T·∫£i d·ªØ li·ªáu t·ª´ CSV
            data = pd.read_csv(file_path)

            # Ki·ªÉm tra n·∫øu d·ªØ li·ªáu tr·ªëng
            if data.empty:
                st.error(f"Kh√¥ng c√≥ d·ªØ li·ªáu trong file {file_path}")
                return pd.DataFrame()

            # L∆∞u d·ªØ li·ªáu ƒë·ªÉ t√°i s·ª≠ d·ª•ng
            _self._data = data

            # S·ª≠ d·ª•ng logger thay v√¨ print
            _self.logger.info(f"ƒê√£ t·∫£i d·ªØ li·ªáu: {data.shape[0]} d√≤ng v√† {data.shape[1]} c·ªôt")
            return data

        except FileNotFoundError as e:
            st.error(str(e))
            return pd.DataFrame()
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ {file_path}: {e}")
            return pd.DataFrame()

    @st.cache_data
    def preprocess_data(_self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho ph√¢n t√≠ch v√† m√¥ h√¨nh h√≥a
        """
        try:
            # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh c·∫£nh b√°o c·ªßa Pandas
            df = data.copy()

            # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ d·ªÖ s·ª≠ d·ª•ng (n·∫øu ch∆∞a c√≥)
            column_mapping = {
                'area (m2)': 'area_m2',
                'street (m)': 'street_width_m'
            }

            # ƒê·∫£m b·∫£o ch√∫ng ta c√≥ c·∫£ c√°c c·ªôt c≈© v√† m·ªõi
            for old_name, new_name in column_mapping.items():
                if old_name in df.columns:
                    # N·∫øu c·ªôt c≈© t·ªìn t·∫°i, t·∫°o c·ªôt m·ªõi d·ª±a tr√™n n√≥
                    df[new_name] = df[old_name]
                elif new_name not in df.columns and old_name not in df.columns:
                    # N·∫øu c·∫£ hai c·ªôt ƒë·ªÅu kh√¥ng t·ªìn t·∫°i, hi·ªÉn th·ªã l·ªói
                    _self.logger.warning(f"Kh√¥ng t√¨m th·∫•y c·ªôt {old_name} ho·∫∑c {new_name} trong d·ªØ li·ªáu")

            # X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
            numeric_cols = ["area (m2)", "bedroom_num", "floor_num", "toilet_num", "livingroom_num", "street (m)"]
            for col in numeric_cols:
                if col in df:
                    # Thay th·∫ø -1 (gi√° tr·ªã thi·∫øu) b·∫±ng gi√° tr·ªã trung v·ªã
                    median_val = df[df[col] != -1][col].median()
                    df[col] = df[col].replace(-1, median_val)

            # Chuy·ªÉn ƒë·ªïi logarithm cho gi√° (n·∫øu c√≥ c·ªôt gi√°)
            if 'price_per_m2' in df.columns:
                df['price_log'] = np.log1p(df['price_per_m2'])

            # T√≠nh gi√° tr√™n m√©t vu√¥ng n·∫øu ch∆∞a c√≥
            if 'price_per_sqm' not in df.columns and 'price' in df.columns and _self._feature_columns['area'] in df.columns:
                df['price_per_sqm'] = df['price'] / df[_self._feature_columns['area']]

            # L·ªçc c√°c d√≤ng v·ªõi gi√° tr·ªã l·ªói
            if 'price' in df.columns:
                df = df[df['price'] > 0]

            # ƒê·∫£m b·∫£o c√°c c·ªôt s·ªë c√≥ ki·ªÉu d·ªØ li·ªáu ƒë√∫ng
            numeric_cols = [_self._feature_columns['area'], 'price'] if 'price' in df.columns else [_self._feature_columns['area']]
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            _self.logger.info(f"Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ho√†n t·∫•t: {df.shape[0]} d√≤ng, {df.shape[1]} c·ªôt")
            return df

        except Exception as e:
            _self.logger.error(f"L·ªói khi ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu: {e}")
            return data  # Tr·∫£ v·ªÅ d·ªØ li·ªáu g·ªëc n·∫øu c√≥ l·ªói

    # MARK: - Chuy·ªÉn ƒë·ªïi Spark

    @st.cache_resource
    def convert_to_spark(_self, data: pd.DataFrame):
        """
        Chuy·ªÉn ƒë·ªïi DataFrame pandas sang DataFrame Spark
        """
        if _self._spark_df is not None:
            return _self._spark_df

        try:
            # Ki·ªÉm tra xem Spark c√≥ kh·∫£ d·ª•ng kh√¥ng
            spark = _self.get_spark_session_cached()
            if spark is None:
                _self.logger.warning("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
                return None

            # For debugging - commented out
            # print(f"C√°c c·ªôt trong d·ªØ li·ªáu g·ªëc tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi: {data.columns.tolist()}")

            # ƒê·∫£m b·∫£o d·ªØ li·ªáu c√≥ t·∫•t c·∫£ c√°c c·ªôt c·∫ßn thi·∫øt (c·∫£ t√™n c≈© v√† m·ªõi)
            if 'area (m2)' in data.columns and 'area_m2' not in data.columns:
                data['area_m2'] = data['area (m2)'].copy()
            if 'street (m)' in data.columns and 'street_width_m' not in data.columns:
                data['street_width_m'] = data['street (m)'].copy()

            # Chuy·ªÉn ƒë·ªïi DataFrame pandas sang DataFrame Spark
            spark_df = spark.createDataFrame(data)
            _self._spark_df = spark_df
            return spark_df

        except Exception as e:
            _self.logger.error(f"L·ªói khi chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang Spark: {e}")
            return None

    # MARK: - Hu·∫•n luy·ªán m√¥ h√¨nh

    @st.cache_resource
    def get_spark_session_cached(_self):
        """
        Phi√™n b·∫£n c√≥ cache c·ªßa h√†m kh·ªüi t·∫°o Spark v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u v√† x·ª≠ l√Ω l·ªói.
        """
        try:
            # S·ª≠ d·ª•ng ti·ªán √≠ch Spark ƒë√£ c·∫•u h√¨nh ƒë·ªÉ gi·∫£m thi·ªÉu c·∫£nh b√°o
            spark = get_spark_session(
                app_name="VNRealEstatePricePrediction",
                enable_hive=False
            )
            # Ki·ªÉm tra k·∫øt n·ªëi ƒë·ªÉ ƒë·∫£m b·∫£o Spark ho·∫°t ƒë·ªông
            spark.sparkContext.parallelize([1]).collect()
            _self._spark = spark
            return spark
        except Exception as e:
            _self.logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark: {e}")
            return None

    @st.cache_resource
    def train_model(_self, _spark_df=None, force_retrain=False):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ d·ª± ƒëo√°n gi√° b·∫•t ƒë·ªông s·∫£n
        """
        # Ki·ªÉm tra xem metrics c√≥ t·ªìn t·∫°i trong session state kh√¥ng
        if metrics_exist() and not force_retrain:
            # L·∫•y metrics t·ª´ session state
            session_metrics = get_model_metrics()
            _self.logger.info("S·ª≠ d·ª•ng metrics t·ª´ session state: R¬≤ = {:.4f}, RMSE = {:.4f}".format(
                session_metrics.get('r2', 0.0), session_metrics.get('rmse', 0.0)
            ))
            # C·∫≠p nh·∫≠t c√°c metrics c·ªßa DataService t·ª´ session state
            _self._r2 = session_metrics.get('r2', 0.0)
            _self._accuracy = _self._r2  # ƒê·∫£m b·∫£o t∆∞∆°ng th√≠ch ng∆∞·ª£c
            _self._rmse = session_metrics.get('rmse', 0.0)
            return _self._model, _self._accuracy, _self._rmse

        # N·∫øu ƒë√£ c√≥ m√¥ h√¨nh v√† kh√¥ng b·∫Øt bu·ªôc hu·∫•n luy·ªán l·∫°i, tr·∫£ v·ªÅ m√¥ h√¨nh v√† c√°c ch·ªâ s·ªë hi·ªán t·∫°i
        if _self._model is not None and not force_retrain:
            return _self._model, _self._accuracy, _self._rmse

        # N·∫øu ch∆∞a c√≥ d·ªØ li·ªáu, t·∫£i d·ªØ li·ªáu
        if _self._data is None:
            _self._data = _self.load_data()

        # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, tr·∫£ v·ªÅ None
        if _self._data is None or _self._data.empty:
            _self.logger.error("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh")
            return None, 0, 0

        # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
        processed_data = _self.preprocess_data(_self._data)

        # Kh·ªüi t·∫°o SparkSession
        spark = _self.get_spark_session_cached()

        # N·∫øu spark_df kh√¥ng ƒë∆∞·ª£c cung c·∫•p, t·∫°o m·ªõi
        if _spark_df is None:
            # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang Spark DataFrame
            _spark_df = _self.convert_to_spark(processed_data)

        # Ki·ªÉm tra n·∫øu data_spark l√† None (khi Spark kh√¥ng kh·∫£ d·ª•ng)
        if _spark_df is None:
            # Thi·∫øt l·∫≠p gi√° tr·ªã metrics m·∫∑c ƒë·ªãnh
            _self._rmse = 0.0
            _self._r2 = 0.0

            # S·ª≠ d·ª•ng fallback mode v·ªõi scikit-learn
            try:
                # S·ª≠ d·ª•ng bi·∫øn to√†n c·ª•c ƒë·ªÉ ki·ªÉm tra scikit-learn
                # sklearn_available ƒë√£ ƒë∆∞·ª£c khai b√°o ·ªü ƒë·∫ßu t·∫≠p tin

                if sklearn_available:
                    # Chu·∫©n b·ªã d·ªØ li·ªáu cho scikit-learn
                    X = processed_data.drop(['price_per_sqm', 'price_million_vnd'], axis=1, errors='ignore')

                    # Ch·ªçn label (gi√°) - ∆∞u ti√™n price_per_sqm n·∫øu c√≥
                    if 'price_per_sqm' in processed_data.columns:
                        y = processed_data['price_per_sqm']
                    elif 'price' in processed_data.columns:
                        y = processed_data['price']
                    else:
                        raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt gi√° (price ho·∫∑c price_per_sqm) trong d·ªØ li·ªáu")

                    # T·∫°o b·ªô l·ªçc cho c√°c c·ªôt s·ªë (lo·∫°i b·ªè c·ªôt object/categorical)
                    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
                    X = X[numeric_cols]  # Ch·ªâ s·ª≠ d·ª•ng c√°c c·ªôt s·ªë

                    # Chia d·ªØ li·ªáu train/test
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

                    # Hu·∫•n luy·ªán m√¥ h√¨nh
                    fallback_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
                    fallback_model.fit(X_train, y_train)

                    # ƒê√°nh gi√° m√¥ h√¨nh
                    y_pred = fallback_model.predict(X_test)
                    r2 = r2_score(y_test, y_pred)
                    rmse = mean_squared_error(y_test, y_pred, squared=False)

                    # L∆∞u metrics
                    _self._model = fallback_model
                    _self._accuracy = r2
                    _self._r2 = r2
                    _self._rmse = rmse
                    _self._using_fallback = True
                    _self._feature_importance = dict(zip(X.columns, fallback_model.feature_importances_))

                    # L∆∞u metrics v√†o session state
                    save_model_metrics(r2=r2, rmse=rmse)

                    _self.logger.info(f"Hu·∫•n luy·ªán m√¥ h√¨nh d·ª± ph√≤ng v·ªõi scikit-learn, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}")
                    return fallback_model, r2, rmse
                else:
                    # S·ª≠ d·ª•ng ch·∫ø ƒë·ªô d·ª± ph√≤ng r·∫•t ƒë∆°n gi·∫£n khi kh√¥ng c√≥ scikit-learn
                    _self._using_fallback = True
                    _self.logger.warning("‚ùó Kh√¥ng th·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh n√¢ng cao. S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p t√≠nh trung b√¨nh ƒë∆°n gi·∫£n.")
                    return None, 0.0, 0.0
            except Exception as e:
                st.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh d·ª± ph√≤ng: {e}")
                # ƒê√£ thi·∫øt l·∫≠p gi√° tr·ªã m·∫∑c ƒë·ªãnh cho metrics ·ªü tr√™n
                return None, 0.0, 0.0

        # N·∫øu c√≥ Spark DataFrame, hu·∫•n luy·ªán v·ªõi Spark ML
        try:
            # ƒê·ªãnh nghƒ©a c√°c c·ªôt ƒë·ªÉ s·ª≠ d·ª•ng trong m√¥ h√¨nh
            area_column = _self._feature_columns['area']  # 'area (m2)'
            street_column = _self._feature_columns['street']  # 'street (m)'

            # ƒê·∫∑c tr∆∞ng s·ªë
            numeric_features = [area_column, "bedroom_num", "floor_num", "toilet_num", "livingroom_num", street_column]

            # Ch·ªâ s·ª≠ d·ª•ng c√°c c·ªôt t·ªìn t·∫°i trong d·ªØ li·ªáu
            numeric_features = [col for col in numeric_features if col in _spark_df.columns]

            # ƒê·∫∑c tr∆∞ng ph√¢n lo·∫°i
            categorical_features = ["category", "direction", "liability", "district", "city_province"]

            # Lo·∫°i tr·ª´ c√°c ƒë·∫∑c tr∆∞ng kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu
            categorical_features = [col for col in categorical_features if col in processed_data.columns]

            # T·∫°o onehot encoding cho c√°c bi·∫øn ph√¢n lo·∫°i
            indexers = [StringIndexer(inputCol=col, outputCol=col+"_index", handleInvalid="keep")
                        for col in categorical_features]

            encoders = [OneHotEncoder(inputCol=col+"_index", outputCol=col+"_encoded")
                        for col in categorical_features]

            # G·ªôp t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng ƒë√£ x·ª≠ l√Ω v√†o m·ªôt vector
            assembler_inputs = numeric_features + [col+"_encoded" for col in categorical_features]

            assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features", handleInvalid="skip")

            # T·∫°o chu·∫©n h√≥a d·ªØ li·ªáu
            scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

            # Kh·ªüi t·∫°o m√¥ h√¨nh GBT
            gbt = GBTRegressor(featuresCol="scaled_features", labelCol="price_per_m2", maxIter=10)

            # T·∫°o pipeline
            pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, gbt])

            # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
            train_data, test_data = _spark_df.randomSplit([0.8, 0.2], seed=42)

            # Hu·∫•n luy·ªán m√¥ h√¨nh
            with st.spinner('ƒêang hu·∫•n luy·ªán m√¥ h√¨nh...'):
                model = pipeline.fit(train_data)

            # ƒê√°nh gi√° m√¥ h√¨nh
            predictions = model.transform(test_data)

            # T√≠nh to√°n c√°c ch·ªâ s·ªë ƒë√°nh gi√°
            evaluator = RegressionEvaluator(labelCol="price_per_m2", predictionCol="prediction", metricName="rmse")
            rmse = evaluator.evaluate(predictions)

            evaluator.setMetricName("r2")
            r2 = evaluator.evaluate(predictions)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë√°nh gi√°
            _self._rmse = rmse
            _self._r2 = r2
            _self._accuracy = r2  # For backwards compatibility

            # ƒê√°nh d·∫•u ƒëang s·ª≠ d·ª•ng Spark
            _self._using_fallback = False
            _self._model = model

            # L∆∞u metrics v√†o session state ƒë·ªÉ duy tr√¨ gi·ªØa c√°c views
            save_model_metrics(r2=r2, rmse=rmse)

            # Log model metrics
            _self.logger.info(f"ƒê√£ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi Spark, R¬≤: {r2:.4f}, RMSE: {rmse:.4f}")

            return model, r2, rmse

        except Exception as e:
            _self.logger.error(f"L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh Spark: {e}")
            # Thi·∫øt l·∫≠p gi√° tr·ªã m·∫∑c ƒë·ªãnh cho metrics
            _self._rmse = 0.0
            _self._r2 = 0.0
            return None, 0.0, 0.0

    # MARK: - D·ª± ƒëo√°n gi√°

    @st.cache_data
    def predict_property_price(_self, property_data: Property) -> PredictionResult:
        """
        D·ª± ƒëo√°n gi√° c·ªßa m·ªôt b·∫•t ƒë·ªông s·∫£n s·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán
        """
        try:
            # Ki·ªÉm tra n·∫øu m√¥ h√¨nh d·ª± ph√≤ng scikit-learn ƒë∆∞·ª£c s·ª≠ d·ª•ng
            if _self._using_fallback and _self._fallback_model is not None:
                return _self._predict_price_fallback(property_data)

            # Ki·ªÉm tra n·∫øu m√¥ h√¨nh Spark kh√¥ng kh·∫£ d·ª•ng
            spark = _self.get_spark_session_cached()
            if _self._model is None or spark is None:
                _self.logger.warning("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán ho·∫∑c Spark kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
                return _self._predict_price_fallback(property_data)

            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho d·ª± ƒëo√°n
            property_dict = property_data.to_dict()
            data_copy = {k: [v] for k, v in property_dict.items()}

            # T·∫°o pandas DataFrame
            import pandas as pd
            input_df = pd.DataFrame(data_copy)

            # Sao ch√©p d·ªØ li·ªáu ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn d·ªØ li·ªáu g·ªëc
            data_copy = input_df.copy()

            # X·ª≠ l√Ω c√°c gi√° tr·ªã kh√¥ng t·ªìn t·∫°i
            for col in data_copy.columns:
                if col in ["bedroom_num", "floor_num", "toilet_num", "livingroom_num"]:
                    data_copy[col] = data_copy[col].fillna(-1).astype(int)

            # ƒê·∫£m b·∫£o ch√∫ng ta c√≥ c√°c c·ªôt ƒë√∫ng t√™n ch√≠nh x√°c
            # ƒê·∫£m b·∫£o kh√¥ng s·ª≠ d·ª•ng area_m2 m√† s·ª≠ d·ª•ng 'area (m2)'
            if 'area_m2' in data_copy.columns and 'area (m2)' not in data_copy.columns:
                data_copy['area (m2)'] = data_copy['area_m2'].copy()
                del data_copy['area_m2']
            elif 'area' in data_copy.columns and 'area (m2)' not in data_copy.columns and 'area_m2' not in data_copy.columns:
                data_copy['area (m2)'] = data_copy['area'].copy()

            # ƒê·∫£m b·∫£o kh√¥ng s·ª≠ d·ª•ng street_width_m m√† s·ª≠ d·ª•ng 'street (m)'
            if 'street_width_m' in data_copy.columns and 'street (m)' not in data_copy.columns:
                data_copy['street (m)'] = data_copy['street_width_m'].copy()
                del data_copy['street_width_m']

            # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang Spark DataFrame
            try:
                # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang Spark DataFrame
                spark_df = spark.createDataFrame(data_copy)

                # D·ª± ƒëo√°n gi√° v·ªõi hi·ªáu ·ª©ng hi·ªÉn th·ªã
                with st.spinner('ƒêang d·ª± ƒëo√°n gi√° b·∫•t ƒë·ªông s·∫£n...'):
                    predictions = _self._model.transform(spark_df)

                    # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n
                    prediction_value = predictions.select("prediction").collect()[0][0]

                # N·∫øu k·∫øt qu·∫£ h·ª£p l·ªá
                if prediction_value is not None:
                    # T√≠nh gi√° tr√™n m√©t vu√¥ng
                    area = property_data.area
                    predicted_price_per_sqm = prediction_value
                    predicted_price = prediction_value * area

                    # X√°c th·ª±c v√† l·∫•y d·ªØ li·ªáu so s√°nh khu v·ª±c
                    # ƒê·∫£m b·∫£o r·∫±ng location l√† h·ª£p l·ªá tr∆∞·ªõc khi s·ª≠ d·ª•ng
                    if hasattr(property_data, 'location') and property_data.location is not None:
                        location = property_data.location
                    else:
                        location = ""
                        _self.logger.warning("V·ªã tr√≠ kh√¥ng h·ª£p l·ªá ho·∫∑c tr·ªëng, s·ª≠ d·ª•ng chu·ªói r·ªóng")

                    comparison_data = _self._get_area_comparison(location)

                    # T·∫°o k·∫øt qu·∫£ d·ª± ƒëo√°n
                    result = PredictionResult.create(
                        predicted_price=predicted_price,
                        predicted_price_per_sqm=predicted_price_per_sqm,
                        property_details=property_data.to_dict(),
                        comparison_data=comparison_data
                    )

                    return result
                else:
                    # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng n·∫øu gi√° tr·ªã d·ª± ƒëo√°n l√† None
                    st.warning("K·∫øt qu·∫£ d·ª± ƒëo√°n kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng.")
                    return _self._predict_price_fallback(property_data)
            except Exception as e:
                st.warning(f"L·ªói khi d·ª± ƒëo√°n v·ªõi Spark: {e}. S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng.")
                return _self._predict_price_fallback(property_data)
        except Exception as e:
            _self.logger.error(f"L·ªói khi d·ª± ƒëo√°n gi√°: {e}")
            # Fallback to traditional calculation
            return _self._predict_price_fallback(property_data)

    # MARK: - Ph∆∞∆°ng ph√°p d·ª± ph√≤ng

    def _predict_price_fallback(_self, property_data: Property) -> PredictionResult:
        """
        Ph∆∞∆°ng ph√°p d·ª± ph√≤ng ƒë∆°n gi·∫£n cho vi·ªác d·ª± ƒëo√°n gi√° khi m√¥ h√¨nh Spark kh√¥ng kh·∫£ d·ª•ng
        """
        try:
            import time

            # L·∫•y c√°c thu·ªôc t√≠nh c∆° b·∫£n, v·ªõi ki·ªÉm tra an to√†n
            area = getattr(property_data, 'area', 0)
            if area <= 0:
                area = 50  # Di·ªán t√≠ch m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ ho·∫∑c kh√¥ng h·ª£p l·ªá

            location = getattr(property_data, 'location', '')
            num_rooms = getattr(property_data, 'num_rooms', 2)
            year_built = getattr(property_data, 'year_built', 2010)
            legal_status = getattr(property_data, 'legal_status', '')

            # Hi·ªáu ·ª©ng loading ƒë·ªÉ t·∫°o tr·∫£i nghi·ªám t·ªët h∆°n
            with st.spinner('ƒêang t√≠nh to√°n gi√° b·∫•t ƒë·ªông s·∫£n...'):
                # T·∫°o ch√∫t delay cho hi·ªáu ·ª©ng
                time.sleep(0.8)

                # Gi√° c∆° b·∫£n d·ª±a tr√™n v·ªã tr√≠
                # Gi√° m·∫∑c ƒë·ªãnh cho m·ªói m2 l√† 30 tri·ªáu VND
                base_price_per_sqm = 30000000

                # ƒêi·ªÅu ch·ªânh theo v·ªã tr√≠
                location_factor = 1.0
                if location and any(premium_area in location.lower() for premium_area in ['qu·∫≠n 1', 'qu·∫≠n 3', 'th·ªß ƒë·ª©c', 'b√¨nh th·∫°nh', 'ph√∫ nhu·∫≠n']):
                    location_factor = 1.5  # Khu v·ª±c cao c·∫•p
                elif location and any(mid_area in location.lower() for mid_area in ['qu·∫≠n 2', 'qu·∫≠n 4', 'qu·∫≠n 7', 'qu·∫≠n 10', 't√¢n b√¨nh']):
                    location_factor = 1.2  # Khu v·ª±c trung b√¨nh
                elif location and any(low_area in location.lower() for low_area in ['qu·∫≠n 8', 'qu·∫≠n 9', 'qu·∫≠n 12', 'b√¨nh t√¢n', 't√¢n ph√∫']):
                    location_factor = 0.9  # Khu v·ª±c th·∫•p h∆°n

                # ƒêi·ªÅu ch·ªânh theo s·ªë ph√≤ng ng·ªß
                room_factor = 1.0
                if num_rooms <= 1:
                    room_factor = 0.9
                elif num_rooms == 3:
                    room_factor = 1.1
                elif num_rooms >= 4:
                    room_factor = 1.2

                # ƒêi·ªÅu ch·ªânh theo nƒÉm x√¢y d·ª±ng
                age_factor = 1.0
                current_year = 2025
                age = current_year - year_built if year_built > 0 else 10
                if age < 5:
                    age_factor = 1.15  # M·ªõi x√¢y
                elif age < 10:
                    age_factor = 1.05  # Kh√° m·ªõi
                elif age > 20:
                    age_factor = 0.9   # C≈©

                # ƒêi·ªÅu ch·ªânh theo t√¨nh tr·∫°ng ph√°p l√Ω
                legal_factor = 1.0
                if legal_status and any(good_status in legal_status.lower() for good_status in ['s·ªï ƒë·ªè', 's·ªï h·ªìng', 'ch√≠nh ch·ªß']):
                    legal_factor = 1.1

                # T√≠nh gi√° cu·ªëi c√πng
                adjusted_price_per_sqm = base_price_per_sqm * location_factor * room_factor * age_factor * legal_factor
                total_price = adjusted_price_per_sqm * area

            # T·∫°o d·ªØ li·ªáu so s√°nh (ho·∫∑c ƒë·ªÉ tr·ªëng n·∫øu kh√¥ng c·∫ßn thi·∫øt)
            comparison_data = {
                'area_avg_price': adjusted_price_per_sqm * 0.9,  # Gi·∫£ l·∫≠p gi√° trung b√¨nh khu v·ª±c
                'location': location
            }

            # T·∫°o v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
            result = PredictionResult.create(
                predicted_price=total_price,
                predicted_price_per_sqm=adjusted_price_per_sqm,
                property_details=property_data.to_dict(),
                comparison_data=comparison_data
            )

            return result

        except Exception as e:
            # X·ª≠ l√Ω l·ªói v√† tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh an to√†n
            st.error(f"L·ªói khi d·ª± ƒëo√°n gi√° (ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n): {str(e)}")

            # T√≠nh to√°n gi√° m·∫∑c ƒë·ªãnh d·ª±a tr√™n di·ªán t√≠ch
            default_area = getattr(property_data, 'area', 50)
            if default_area <= 0:
                default_area = 50

            default_price_per_sqm = 30000000  # 30 tri·ªáu VND/m2
            default_price = default_price_per_sqm * default_area

            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ m·∫∑c ƒë·ªãnh
            return PredictionResult.create(
                predicted_price=default_price,
                predicted_price_per_sqm=default_price_per_sqm,
                property_details=property_data.to_dict(),
                comparison_data={}
            )

    # MARK: - Ph√¢n t√≠ch so s√°nh

    def _get_area_comparison(self, location: str) -> Dict[str, Any]:
        """
        L·∫•y d·ªØ li·ªáu so s√°nh gi√° cho m·ªôt v·ªã tr√≠
        """
        data = self._data
        if data is None or data.empty:
            return {}

        # Ki·ªÉm tra xem location c√≥ gi√° tr·ªã h·ª£p l·ªá kh√¥ng
        if location is None or not isinstance(location, str) or location.strip() == '':
            self.logger.warning("V·ªã tr√≠ kh√¥ng h·ª£p l·ªá ho·∫∑c tr·ªëng, s·ª≠ d·ª•ng to√†n b·ªô d·ªØ li·ªáu")
            location_data = data
        else:
            try:
                # Filter by location if possible
                location_filter = location.lower()
                # Ki·ªÉm tra xem c·ªôt 'location' c√≥ t·ªìn t·∫°i trong d·ªØ li·ªáu kh√¥ng
                if 'location' in data.columns:
                    location_data = data[data['location'].str.lower().str.contains(location_filter)]
                else:
                    self.logger.warning("Kh√¥ng t√¨m th·∫•y c·ªôt 'location' trong d·ªØ li·ªáu")
                    location_data = data
            except Exception as e:
                self.logger.error(f"L·ªói khi l·ªçc d·ªØ li·ªáu theo v·ªã tr√≠: {e}")
                location_data = data

        # If no data for location, use all data
        if location_data.empty:
            location_data = data

        # T√≠nh to√°n th·ªëng k√™
        avg_price = location_data['price'].mean()
        min_price = location_data['price'].min()
        max_price = location_data['price'].max()

        avg_price_per_sqm = location_data['price_per_sqm'].mean()
        min_price_per_sqm = location_data['price_per_sqm'].min()
        max_price_per_sqm = location_data['price_per_sqm'].max()

        return {
            'location': location,
            'avg_price': avg_price,
            'min_price': min_price,
            'max_price': max_price,
            'avg_price_per_sqm': avg_price_per_sqm,
            'min_price_per_sqm': min_price_per_sqm,
            'max_price_per_sqm': max_price_per_sqm
        }

    # MARK: - ƒê·∫∑c tr∆∞ng quan tr·ªçng

    def get_feature_importance(self) -> Dict[str, float]:
        """
        L·∫•y m·ª©c ƒë·ªô quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng trong m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán
        """
        try:
            if self._model is None:
                return {}

            # Tr√≠ch xu·∫•t m√¥ h√¨nh GBT t·ª´ pipeline
            gbt_model = self._model.stages[-1]

            # L·∫•y m·ª©c ƒë·ªô quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng
            feature_importance = gbt_model.featureImportances

            # L·∫•y t√™n c√°c ƒë·∫∑c tr∆∞ng
            feature_cols = [
                'area',
                'num_rooms',
                'year_built',
                'location_encoded',
                'legal_status_encoded',
                'house_direction_encoded'
            ]

            # T·∫°o t·ª´ ƒëi·ªÉn m·ª©c ƒë·ªô quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng
            importance_dict = {
                feature: importance
                for feature, importance in zip(feature_cols, feature_importance.toArray())
            }

            # S·∫Øp x·∫øp theo m·ª©c ƒë·ªô quan tr·ªçng (gi·∫£m d·∫ßn)
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))

        except Exception as e:
            print(f"L·ªói khi tr√≠ch xu·∫•t t·∫ßm quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng: {e}")
            return {}
